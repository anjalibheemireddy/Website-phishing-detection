{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a0b514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.facebook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dbe3b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import joblib  # For loading your trained model\n",
    "import re\n",
    "\n",
    "# Load your pre-trained model and feature extraction functions here\n",
    "# ...\n",
    "# Initialize the features dictionary\n",
    "features = {}\n",
    "\n",
    "# Function to extract features from a URL\n",
    "def extract_features(url):\n",
    "    try:\n",
    "        # Fetch HTML content\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "\n",
    "        # Parse HTML content\n",
    "        parsed_content = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Extract features based on your feature extraction functions\n",
    "        # Example: Feature extraction logic for length_url\n",
    "        length_url = len(url)\n",
    "\n",
    "        # Add more feature extraction logic for other features...\n",
    "        # Example:\n",
    "        nb_dots = url.count('.')\n",
    "        nb_hyphens = url.count('-')\n",
    "        ip = int(bool(re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url)))  # Check for the presence of an IP address in the URL\n",
    "        nb_dots = url.count('.')\n",
    "        nb_hyphens = url.count('-')\n",
    "        nb_at = url.count('@')\n",
    "        nb_qm = url.count('?')\n",
    "        nb_and = url.count('&')\n",
    "        nb_or = url.count('|')\n",
    "        nb_eq = url.count('=')\n",
    "        nb_underscore = url.count('_')\n",
    "        nb_tilde = url.count('~')\n",
    "        nb_percent = url.count('%')\n",
    "        nb_slash = url.count('/')\n",
    "        nb_star = url.count('*')\n",
    "        nb_colon = url.count(':')\n",
    "        nb_comma = url.count(',')\n",
    "        nb_semicolon = url.count(';')\n",
    "        nb_dollar = url.count('$')\n",
    "        nb_space = url.count(' ')\n",
    "        nb_www = url.count('www')\n",
    "        nb_com = url.count('.com')\n",
    "        nb_dslash = url.count('//')\n",
    "        http_in_path = int('http' in url)\n",
    "        https_token = int('https' in url)\n",
    "#         ratio_digits_url = sum(c.isdigit() for c in url) / length_url\n",
    "#         ratio_digits_host = sum(c.isdigit() for c in parsed_content.netloc) / length_hostname\n",
    "\n",
    "        # Return a dictionary of features\n",
    "        features[\"length_url\"] = length_url\n",
    "        features[\"nb_dots\"] = nb_dots\n",
    "        features[\"nb_hyphens\"] = nb_hyphens\n",
    "        features[\"nb_at\"] = nb_at\n",
    "        features[\"nb_qm\"] = nb_qm\n",
    "        features[\"nb_and\"] = nb_and\n",
    "        features[\"nb_or\"] = nb_or\n",
    "        features[\"nb_eq\"] = nb_eq\n",
    "        features[\"nb_underscore\"] = nb_underscore\n",
    "        features[\"nb_tilde\"] = nb_tilde\n",
    "        features[\"nb_percent\"] = nb_percent\n",
    "        features[\"nb_slash\"] = nb_slash\n",
    "        features[\"nb_star\"] = nb_star\n",
    "        features[\"nb_colon\"] = nb_colon\n",
    "        features[\"nb_comma\"] = nb_comma\n",
    "        features[\"nb_semicolon\"] = nb_semicolon\n",
    "        features[\"nb_dollar\"] = nb_dollar\n",
    "        features[\"nb_space\"] = nb_space\n",
    "        features[\"nb_www\"] = nb_www\n",
    "        features[\"nb_com\"] = nb_com\n",
    "        features[\"nb_dslash\"] = nb_dslash\n",
    "        features[\"http_in_path\"] = http_in_path\n",
    "        features[\"https_token\"] = https_token\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75f4947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length_url': 23, 'nb_dots': 2, 'nb_hyphens': 0, 'nb_at': 0, 'nb_qm': 0, 'nb_and': 0, 'nb_or': 0, 'nb_eq': 0, 'nb_underscore': 0, 'nb_tilde': 0, 'nb_percent': 0, 'nb_slash': 2, 'nb_star': 0, 'nb_colon': 1, 'nb_comma': 0, 'nb_semicolon': 0, 'nb_dollar': 0, 'nb_space': 0, 'nb_www': 1, 'nb_com': 1, 'nb_dslash': 1, 'http_in_path': 1, 'https_token': 0}\n"
     ]
    }
   ],
   "source": [
    "extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48571b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a8adf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import tldextract\n",
    "\n",
    "# Fetch HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse HTML content\n",
    "parsed_content = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Function to check for the presence of Punycode in the domain\n",
    "def extract_punycode(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    return 1 if any(ord(char) > 127 for char in extracted.domain) else 0\n",
    "\n",
    "# Function to extract the 'port' feature\n",
    "def extract_port(url):\n",
    "    return 1 if \":\" in url else 0\n",
    "\n",
    "# Function to extract the 'tld_in_path' feature\n",
    "def extract_tld_in_path(url):\n",
    "#     path = tldextract.extract(url).path\n",
    "    return 1 if url.endswith((\".com\", \".org\", \".net\", \".edu\", \".gov\", \".mil\")) else 0\n",
    "\n",
    "\n",
    "# Function to extract the 'tld_in_subdomain' feature\n",
    "def extract_tld_in_subdomain(url):\n",
    "    subdomain = tldextract.extract(url).subdomain\n",
    "    return 1 if tldextract.extract(subdomain).suffix else 0\n",
    "\n",
    "# Function to extract the 'abnormal_subdomain' feature (example check)\n",
    "def extract_abnormal_subdomain(url):\n",
    "    subdomain = tldextract.extract(url).subdomain\n",
    "    # Example check: Consider \"www\" as not abnormal, everything else as abnormal\n",
    "    return 0 if subdomain == \"www\" else 1\n",
    "\n",
    "# Function to extract the 'nb_subdomains' feature\n",
    "def extract_nb_subdomains(url):\n",
    "    subdomains = tldextract.extract(url).subdomain\n",
    "    return len(subdomains.split('.'))\n",
    "\n",
    "# Function to extract the 'prefix_suffix' feature\n",
    "def extract_prefix_suffix(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    return 1 if domain.startswith('-') or domain.endswith('-') else 0\n",
    "\n",
    "# Function to extract the 'random_domain' feature (example check)\n",
    "def extract_random_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    # Example check: Consider \"example\" as not random, everything else as random\n",
    "    return 0 if domain == \"example\" else 1\n",
    "\n",
    "# Function to extract the 'shortening_service' feature (example check)\n",
    "def extract_shortening_service(url):\n",
    "    # Example check: Detect common URL shortening services like \"bit.ly\"\n",
    "    shortening_services = [\"bit.ly\", \"t.co\", \"tinyurl\"]\n",
    "    return 1 if any(service in url for service in shortening_services) else 0\n",
    "\n",
    "# Function to extract the 'nb_redirection' feature\n",
    "def extract_nb_redirections(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    redirections = soup.find_all(\"meta\", attrs={\"http-equiv\": \"refresh\"})\n",
    "    return len(redirections)\n",
    "\n",
    "# Function to extract the 'nb_external_redirection' feature (example check)\n",
    "def extract_nb_external_redirections(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    redirections = soup.find_all(\"meta\", attrs={\"http-equiv\": \"refresh\"})\n",
    "    # Example check: Consider external redirections if they contain \"http\" or \"https\"\n",
    "    return sum(1 for redirection in redirections if \"http\" in redirection[\"content\"])\n",
    "\n",
    "# Function to extract the 'length_words_raw' feature (example calculation)\n",
    "def extract_length_words_raw(html_content):\n",
    "    words = re.findall(r'\\w+', html_content)\n",
    "    return len(words)\n",
    "\n",
    "# Function to extract the 'char_repeat' feature (example calculation)\n",
    "def extract_char_repeat(html_content):\n",
    "    repeated_chars = re.findall(r'((\\w)\\2{2,})', html_content)\n",
    "    return len(repeated_chars)\n",
    "\n",
    "# Function to extract the 'shortest_words_raw' feature (example calculation)\n",
    "def extract_shortest_words_raw(html_content):\n",
    "    words = re.findall(r'\\w+', html_content)\n",
    "    shortest_word = min(words, key=len, default='')\n",
    "    return len(shortest_word)\n",
    "# Function to extract the 'longest_words_raw' feature (example calculation)\n",
    "def extract_longest_words_raw(html_content):\n",
    "    words = re.findall(r'\\w+', html_content)\n",
    "    longest_word = max(words, key=len, default='')\n",
    "    return len(longest_word)\n",
    "\n",
    "# Function to extract the 'avg_words_raw' feature (example calculation)\n",
    "def extract_avg_words_raw(html_content):\n",
    "    words = re.findall(r'\\w+', html_content)\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    return total_word_length / len(words) if len(words) > 0 else 0\n",
    "\n",
    "# Function to extract the 'phish_hints' feature\n",
    "def extract_phish_hints(html_content):\n",
    "    # Example check: Search for phishing-related keywords in the content\n",
    "    phishing_keywords = [\"phish\", \"fraud\", \"scam\", \"spoof\"]\n",
    "    return 1 if any(keyword in html_content for keyword in phishing_keywords) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50e647c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['punycode'] = extract_punycode(url)\n",
    "features['port'] = extract_port(url)\n",
    "features['tld_in_path'] = extract_tld_in_path(url)\n",
    "features['tld_in_subdomain'] = extract_tld_in_subdomain(url)\n",
    "features['abnormal_subdomain'] = extract_abnormal_subdomain(url)\n",
    "features['nb_subdomains'] = extract_nb_subdomains(url)\n",
    "features['prefix_suffix'] = extract_prefix_suffix(url)\n",
    "features['random_domain'] = extract_random_domain(url)\n",
    "features['shortening_service'] = extract_shortening_service(url)\n",
    "# features['path_extension'] = extract_path_extension(url)\n",
    "features['nb_redirection'] = extract_nb_redirections(html_content)\n",
    "features['nb_external_redirection'] = extract_nb_external_redirections(html_content)\n",
    "features['length_words_raw'] = extract_length_words_raw(html_content)\n",
    "features['char_repeat'] = extract_char_repeat(html_content)\n",
    "features['shortest_words_raw'] = extract_shortest_words_raw(html_content)\n",
    "features['longest_words_raw'] = extract_longest_words_raw(html_content)\n",
    "features['avg_words_raw'] = extract_avg_words_raw(html_content)\n",
    "features['phish_hints'] = extract_phish_hints(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8ca672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_url: 23\n",
      "nb_dots: 2\n",
      "nb_hyphens: 0\n",
      "nb_at: 0\n",
      "nb_qm: 0\n",
      "nb_and: 0\n",
      "nb_or: 0\n",
      "nb_eq: 0\n",
      "nb_underscore: 0\n",
      "nb_tilde: 0\n",
      "nb_percent: 0\n",
      "nb_slash: 2\n",
      "nb_star: 0\n",
      "nb_colon: 1\n",
      "nb_comma: 0\n",
      "nb_semicolon: 0\n",
      "nb_dollar: 0\n",
      "nb_space: 0\n",
      "nb_www: 1\n",
      "nb_com: 1\n",
      "nb_dslash: 1\n",
      "http_in_path: 1\n",
      "https_token: 0\n",
      "punycode: 0\n",
      "port: 1\n",
      "tld_in_path: 1\n",
      "tld_in_subdomain: 0\n",
      "abnormal_subdomain: 0\n",
      "nb_subdomains: 1\n",
      "prefix_suffix: 0\n",
      "random_domain: 1\n",
      "shortening_service: 0\n",
      "nb_redirection: 1\n",
      "nb_external_redirection: 0\n",
      "length_words_raw: 7152\n",
      "char_repeat: 105\n",
      "shortest_words_raw: 1\n",
      "longest_words_raw: 104\n",
      "avg_words_raw: 5.862975391498882\n",
      "phish_hints: 0\n",
      "domain_in_brand: 0\n",
      "brand_in_subdomain: 0\n",
      "suspecious_tld: 0\n",
      "statistical_report: 1\n",
      "nb_hyperlinks: 46\n",
      "nb_extCSS: 6\n"
     ]
    }
   ],
   "source": [
    "# Function to extract the 'domain_in_brand' feature\n",
    "def extract_domain_in_brand(url, brand_name):\n",
    "    return 1 if brand_name in url else 0\n",
    "\n",
    "# Function to extract the 'brand_in_subdomain' feature\n",
    "def extract_brand_in_subdomain(url, brand_name):\n",
    "    subdomain = tldextract.extract(url).subdomain\n",
    "    return 1 if brand_name in subdomain else 0\n",
    "\n",
    "# Function to extract the 'brand_in_path' feature\n",
    "def extract_brand_in_path(url, brand_name):\n",
    "    path = tldextract.extract(url).path\n",
    "    return 1 if brand_name in path else 0\n",
    "\n",
    "# Function to extract the 'suspecious_tld' feature (example check)\n",
    "def extract_suspecious_tld(url):\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    # Example check: Detect suspicious TLDs like \".tk\", \".ml\", \".ga\", \".cf\", \".gq\"\n",
    "    return 1 if tld in [\".tk\", \".ml\", \".ga\", \".cf\", \".gq\"] else 0\n",
    "\n",
    "# Function to extract the 'statistical_report' feature (example check)\n",
    "def extract_statistical_report(html_content):\n",
    "    # Example check: Detect the presence of statistical reports in the content\n",
    "    statistical_keywords = [\"statistics\", \"report\", \"data\", \"analysis\"]\n",
    "    return 1 if any(keyword in html_content for keyword in statistical_keywords) else 0\n",
    "\n",
    "# Function to extract the 'nb_hyperlinks' feature\n",
    "def extract_nb_hyperlinks(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    hyperlinks = soup.find_all(\"a\")\n",
    "    return len(hyperlinks)\n",
    "\n",
    "# Function to extract the 'ratio_intHyperlinks' feature\n",
    "def extract_ratio_intHyperlinks(html_content):\n",
    "    # Example calculation: Calculate the ratio of internal hyperlinks to total hyperlinks\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    hyperlinks = soup.find_all(\"a\")\n",
    "    int_hyperlinks = sum(1 for link in hyperlinks if url_is_internal(link[\"href\"]))\n",
    "    return int_hyperlinks / len(hyperlinks) if len(hyperlinks) > 0 else 0\n",
    "\n",
    "# Function to extract the 'ratio_extHyperlinks' feature\n",
    "def extract_ratio_extHyperlinks(html_content):\n",
    "    # Example calculation: Calculate the ratio of external hyperlinks to total hyperlinks\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    hyperlinks = soup.find_all(\"a\")\n",
    "    ext_hyperlinks = sum(1 for link in hyperlinks if not url_is_internal(link[\"href\"]))\n",
    "    return ext_hyperlinks / len(hyperlinks) if len(hyperlinks) > 0 else 0\n",
    "\n",
    "# Function to extract the 'ratio_nullHyperlinks' feature\n",
    "def extract_ratio_nullHyperlinks(html_content):\n",
    "    # Example calculation: Calculate the ratio of hyperlinks with null (empty) href attributes\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    hyperlinks = soup.find_all(\"a\")\n",
    "    null_hyperlinks = sum(1 for link in hyperlinks if not link.has_attr(\"href\") or not link[\"href\"])\n",
    "    return null_hyperlinks / len(hyperlinks) if len(hyperlinks) > 0 else 0\n",
    "\n",
    "# Function to extract the 'nb_extCSS' feature\n",
    "def extract_nb_extCSS(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    ext_css = soup.find_all(\"link\", attrs={\"rel\": \"stylesheet\"})\n",
    "    return len(ext_css)\n",
    "\n",
    "# ... Add functions for the remaining features similarly ...\n",
    "\n",
    "# Add the extracted features to the dictionary\n",
    "features['domain_in_brand'] = extract_domain_in_brand(url, \"example_brand\")\n",
    "features['brand_in_subdomain'] = extract_brand_in_subdomain(url, \"example_brand\")\n",
    "# features['brand_in_path'] = extract_brand_in_path(url, \"example_brand\")\n",
    "features['suspecious_tld'] = extract_suspecious_tld(url)\n",
    "features['statistical_report'] = extract_statistical_report(html_content)\n",
    "features['nb_hyperlinks'] = extract_nb_hyperlinks(html_content)\n",
    "# features['ratio_intHyperlinks'] = extract_ratio_intHyperlinks(html_content)\n",
    "# features['ratio_extHyperlinks'] = extract_ratio_extHyperlinks(html_content)\n",
    "# features['ratio_nullHyperlinks'] = extract_ratio_nullHyperlinks(html_content)\n",
    "features['nb_extCSS'] = extract_nb_extCSS(html_content)\n",
    "\n",
    "# Print the extracted features\n",
    "for feature_name, feature_value in features.items():\n",
    "    print(f\"{feature_name}: {feature_value}\")\n",
    "\n",
    "# Extract the remaining features using similar functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "244ee6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'http://e710z0ear.du.r.appspot.com/c:/users/user/downlo', 'length_url': 23, 'length_hostname': 21.090288713910763, 'ip': 0.15056867891513562, 'nb_dots': 2, 'nb_hyphens': 0, 'nb_at': 0, 'nb_qm': 0, 'nb_and': 0, 'nb_or': 0, 'nb_eq': 0, 'nb_underscore': 0, 'nb_tilde': 0, 'nb_percent': 0, 'nb_slash': 2, 'nb_star': 0, 'nb_colon': 1, 'nb_comma': 0, 'nb_semicolumn': 0.06229221347331584, 'nb_dollar': 0, 'nb_space': 0, 'nb_www': 1, 'nb_com': 1, 'nb_dslash': 1, 'http_in_path': 1, 'https_token': 0, 'ratio_digits_url': 0.0531373016455818, 'ratio_digits_host': 0.02502370947016623, 'punycode': 0, 'port': 1, 'tld_in_path': 1, 'tld_in_subdomain': 0, 'abnormal_subdomain': 0, 'nb_subdomains': 1, 'prefix_suffix': 0, 'random_domain': 1, 'shortening_service': 0, 'path_extension': 0.00017497812773403323, 'nb_redirection': 1, 'nb_external_redirection': 0, 'length_words_raw': 7152, 'char_repeat': 105, 'shortest_words_raw': 1, 'shortest_word_host': 5.019772528433946, 'shortest_word_path': 2.3989501312335957, 'longest_words_raw': 104, 'longest_word_host': 10.467979002624672, 'longest_word_path': 10.561504811898512, 'avg_words_raw': 5.862975391498882, 'avg_word_host': 7.678074521956081, 'avg_word_path': 5.09242470192406, 'phish_hints': 0, 'domain_in_brand': 0, 'brand_in_subdomain': 0, 'brand_in_path': 0.004899387576552931, 'suspecious_tld': 0, 'statistical_report': 1, 'nb_hyperlinks': 46, 'ratio_intHyperlinks': 0.6024572495056868, 'ratio_extHyperlinks': 0.2767203532939633, 'ratio_nullHyperlinks': 0, 'nb_extCSS': 6, 'ratio_intRedirection': 0, 'ratio_extRedirection': 0.15892561547322834, 'ratio_intErrors': 0, 'ratio_extErrors': 0.06246862846579178, 'login_form': 0.06360454943132109, 'external_favicon': 0.44216972878390204, 'links_in_tags': 51.97821079087927, 'submit_email': 0, 'ratio_intMedia': 42.870443634443305, 'ratio_extMedia': 23.2362930234965, 'sfh': 0, 'iframe': 0.0013123359580052493, 'popup_window': 0.006036745406824147, 'safe_anchor': 37.063921731737096, 'onmouseover': 0.001137357830271216, 'right_clic': 0.0013998250218722659, 'empty_title': 0.1247594050743657, 'domain_in_title': 0.7758530183727034, 'domain_with_copyright': 0.4395450568678915, 'whois_registered_domain': 0.07287839020122484, 'domain_registration_length': 492.53219597550304, 'domain_age': 4062.5437445319335, 'web_traffic': 856756.6433070867, 'dns_record': 0.020122484689413824, 'google_index': 0.5339457567804025, 'page_rank': 3.1857392825896764, 'status': 'legitimate'}\n"
     ]
    }
   ],
   "source": [
    "import statistics  # For computing mode\n",
    "import pandas as pd  # Assuming you have a DataFrame with your dataset\n",
    "\n",
    "# Create a DataFrame with your dataset (replace this with your actual dataset)\n",
    "data = pd.read_csv(\"dataset_phishing.csv\")\n",
    "# Define a function to compute mean for numerical features and mode for categorical features\n",
    "def compute_mean_or_mode(feature_values):\n",
    "    if pd.api.types.is_numeric_dtype(feature_values):\n",
    "        return statistics.mean(feature_values)\n",
    "    else:\n",
    "        return statistics.mode(feature_values)\n",
    "\n",
    "# Create a dictionary 'features1' with computed mean/mode values\n",
    "features1 = {column: compute_mean_or_mode(data[column]) for column in data.columns}\n",
    "features1 = {key: features.get(key, value) for key, value in features1.items()}\n",
    "\n",
    "# # Print the updated 'features' dictionary\n",
    "print(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af2b9080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length_url': 23, 'length_hostname': 21.090288713910763, 'ip': 0.15056867891513562, 'nb_dots': 2, 'nb_hyphens': 0, 'nb_at': 0, 'nb_qm': 0, 'nb_and': 0, 'nb_or': 0, 'nb_eq': 0, 'nb_underscore': 0, 'nb_tilde': 0, 'nb_percent': 0, 'nb_slash': 2, 'nb_star': 0, 'nb_colon': 1, 'nb_comma': 0, 'nb_semicolumn': 0.06229221347331584, 'nb_dollar': 0, 'nb_space': 0, 'nb_www': 1, 'nb_com': 1, 'nb_dslash': 1, 'http_in_path': 1, 'https_token': 0, 'ratio_digits_url': 0.0531373016455818, 'ratio_digits_host': 0.02502370947016623, 'punycode': 0, 'port': 1, 'tld_in_path': 1, 'tld_in_subdomain': 0, 'abnormal_subdomain': 0, 'nb_subdomains': 1, 'prefix_suffix': 0, 'random_domain': 1, 'shortening_service': 0, 'path_extension': 0.00017497812773403323, 'nb_redirection': 1, 'nb_external_redirection': 0, 'length_words_raw': 7152, 'char_repeat': 105, 'shortest_words_raw': 1, 'shortest_word_host': 5.019772528433946, 'shortest_word_path': 2.3989501312335957, 'longest_words_raw': 104, 'longest_word_host': 10.467979002624672, 'longest_word_path': 10.561504811898512, 'avg_words_raw': 5.862975391498882, 'avg_word_host': 7.678074521956081, 'avg_word_path': 5.09242470192406, 'phish_hints': 0, 'domain_in_brand': 0, 'brand_in_subdomain': 0, 'brand_in_path': 0.004899387576552931, 'suspecious_tld': 0, 'statistical_report': 1, 'nb_hyperlinks': 46, 'ratio_intHyperlinks': 0.6024572495056868, 'ratio_extHyperlinks': 0.2767203532939633, 'ratio_nullHyperlinks': 0, 'nb_extCSS': 6, 'ratio_intRedirection': 0, 'ratio_extRedirection': 0.15892561547322834, 'ratio_intErrors': 0, 'ratio_extErrors': 0.06246862846579178, 'login_form': 0.06360454943132109, 'external_favicon': 0.44216972878390204, 'links_in_tags': 51.97821079087927, 'submit_email': 0, 'ratio_intMedia': 42.870443634443305, 'ratio_extMedia': 23.2362930234965, 'sfh': 0, 'iframe': 0.0013123359580052493, 'popup_window': 0.006036745406824147, 'safe_anchor': 37.063921731737096, 'onmouseover': 0.001137357830271216, 'right_clic': 0.0013998250218722659, 'empty_title': 0.1247594050743657, 'domain_in_title': 0.7758530183727034, 'domain_with_copyright': 0.4395450568678915, 'whois_registered_domain': 0.07287839020122484, 'domain_registration_length': 492.53219597550304, 'domain_age': 4062.5437445319335, 'web_traffic': 856756.6433070867, 'dns_record': 0.020122484689413824, 'google_index': 0.5339457567804025, 'page_rank': 3.1857392825896764}\n"
     ]
    }
   ],
   "source": [
    "features1.pop('url')\n",
    "features1.pop('status')\n",
    "print(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6ef128e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL not phishing url.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "input_features = features1\n",
    "# Load the pre-trained model from the pickle file\n",
    "with open('trained_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "# Prepare the input data as a NumPy array\n",
    "input_data = np.array(list(input_features.values())).reshape(1, -1)  # Assuming 1 sample\n",
    "# print(input_data)\n",
    "# Make predictions using the loaded model\n",
    "predicted_class = loaded_model.predict(input_data)\n",
    "\n",
    "# Interpret the predicted class (e.g., 0 for not phishing, 1 for phishing)\n",
    "if predicted_class == 0:\n",
    "    print(\"The URL maybe phishing url.\")\n",
    "else:\n",
    "    print(\"The URL not phishing url.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "28411db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198504e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
